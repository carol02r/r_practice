# this chunk contains code that sets global options for the entire .Rmd.
# we use include=FALSE to suppress it from the top of the document, but it will still appear in the appendix.
knitr::opts_chunk$set(echo = FALSE) # actually set the global chunk options.
# we set echo=FALSE to suppress code such that it by default does not appear throughout the document.
# note: this is different from .Rmd default
### ex4.4
# load files
files <- list.files('transcripts', full.names = TRUE)
cleaned_hearing_docs <- list()
# remove extra text
for (f in files) {
# read file content
raw_text <- readLines(f)
# collapse paragraph lines into one string
full_text <- paste(raw_text, collapse = "\n")
# remove everything before and including the sentence:
# "The hearing will come to order.",
# which marks the beginning of the text
cleaned_text <- sub("(?s).*The hearing will come to order\\.*", "",
full_text, perl = TRUE)
# remove leading and trailing whitespace
cleaned_text <- trimws(cleaned_text)
# remove punctuation and numbers
cleaned_text <- gsub("[[:punct:][:digit:]]", "", cleaned_text)
# separate paragraphs - each one is a document
paragraphs <- unlist(strsplit(cleaned_text, "\n\n"))
cleaned_hearing_docs <- c(cleaned_hearing_docs, paragraphs)}
cleaned_hearing_docs <- unlist(cleaned_hearing_docs)
# create corpus from collection of docs
corpus <- corpus(cleaned_hearing_docs)
# importing necessary packages
library(tidyverse)
library(knitr)
library(kableExtra)
library(RSelenium)
library(rvest)
library(netstat)
library(quanteda)
### ex4.4
# load files
files <- list.files('transcripts', full.names = TRUE)
cleaned_hearing_docs <- list()
# remove extra text
for (f in files) {
# read file content
raw_text <- readLines(f)
# collapse paragraph lines into one string
full_text <- paste(raw_text, collapse = "\n")
# remove everything before and including the sentence:
# "The hearing will come to order.",
# which marks the beginning of the text
cleaned_text <- sub("(?s).*The hearing will come to order\\.*", "",
full_text, perl = TRUE)
# remove leading and trailing whitespace
cleaned_text <- trimws(cleaned_text)
# remove punctuation and numbers
cleaned_text <- gsub("[[:punct:][:digit:]]", "", cleaned_text)
# separate paragraphs - each one is a document
paragraphs <- unlist(strsplit(cleaned_text, "\n\n"))
cleaned_hearing_docs <- c(cleaned_hearing_docs, paragraphs)}
cleaned_hearing_docs <- unlist(cleaned_hearing_docs)
# create corpus from collection of docs
corpus <- corpus(cleaned_hearing_docs)
dfm <- corpus %>%
# remove punctuation and numbers
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
# remove english stopwords
tokens_remove(stopwords("en")) %>%
# create dfm
dfm() %>%
# remove terms appearing in less than 5 docs
dfm_trim(min_termfreq = 5)
length(dfm)
nrows(dfm)
nrow(dfm)
# this chunk contains code that sets global options for the entire .Rmd.
# we use include=FALSE to suppress it from the top of the document, but it will still appear in the appendix.
knitr::opts_chunk$set(echo = FALSE) # actually set the global chunk options.
# we set echo=FALSE to suppress code such that it by default does not appear throughout the document.
# note: this is different from .Rmd default
a <- wiki_senate(118)
### ex2.2
# creating function to automate the url process
# generate suffix based on term number
term_suffix <- function(term) {
# selecting final characters
last_char = str_sub(as.character(term), -1)
last_2_chars = str_sub(as.character(term), -2)
# ending in 1 (st) - exception ending in 11
if (last_char=='1' & last_2_chars!='11'){
term_full <- paste0(term,'st')}
# ending in 2 (nd) - exception ending in 12
else if (last_char=='2' & last_2_chars!='12'){
term_full <- paste0(term,'nd')}
# ending in 3 (rd) - exception ending in 13
else if (last_char=='3' & last_2_chars!='13'){
term_full <- paste0(term,'rd')}
# general case (th)
else{
term_full <- paste0(term,'th')}
return(term_full)}
# function that converts state abbreviations to full names
get_full_state_name <- function(state_abbr) {
# list from chatgpt
state_lookup <- c(
"AL" = "Alabama",
"AK" = "Alaska",
"AZ" = "Arizona",
"AR" = "Arkansas",
"CA" = "California",
"CO" = "Colorado",
"CT" = "Connecticut",
"DE" = "Delaware",
"FL" = "Florida",
"GA" = "Georgia",
"HI" = "Hawaii",
"ID" = "Idaho",
"IL" = "Illinois",
"IN" = "Indiana",
"IA" = "Iowa",
"KS" = "Kansas",
"KY" = "Kentucky",
"LA" = "Louisiana",
"ME" = "Maine",
"MD" = "Maryland",
"MA" = "Massachusetts",
"MI" = "Michigan",
"MN" = "Minnesota",
"MS" = "Mississippi",
"MO" = "Missouri",
"MT" = "Montana",
"NE" = "Nebraska",
"NV" = "Nevada",
"NH" = "New Hampshire",
"NJ" = "New Jersey",
"NM" = "New Mexico",
"NY" = "New York",
"NC" = "North Carolina",
"ND" = "North Dakota",
"OH" = "Ohio",
"OK" = "Oklahoma",
"OR" = "Oregon",
"PA" = "Pennsylvania",
"RI" = "Rhode Island",
"SC" = "South Carolina",
"SD" = "South Dakota",
"TN" = "Tennessee",
"TX" = "Texas",
"UT" = "Utah",
"VT" = "Vermont",
"VA" = "Virginia",
"WA" = "Washington",
"WV" = "West Virginia",
"WI" = "Wisconsin",
"WY" = "Wyoming")
# match 2-letter abbreviation to full term from the list
full_state_name <- state_lookup[state_abbr]
# return full state name or
# the same string as before if abbreviation not found
return(ifelse(is.na(full_state_name), state_abbr, full_state_name))}
# function that extracts all required senator information for a given congress
wiki_senate <- function(term) {
# adding term and suffix to base url
url <- paste0('https://en.wikipedia.org/wiki/List_of_United_States_senators_in_the_',
term_suffix(term) ,'_Congress')
# retrieving content tables from the wikipedia page
wiki_tables <- read_html(url) %>%
html_table()
# the second table contains senator information
info_box <- wiki_tables[[2]]
# no neat tables before congress term 114th
if (term<114){
#using regex to extract elemnts from a string
# format e.g. Name (Party-State)
# name: remove everything including and after first parenthesis
senator_name <- sub('\\s*\\(.*$', '', info_box$`Senator (party-state)`)
# state: grab element after dash and before closing parenthesis
state_list <- sub('^.*\\(.*\\-(.*)\\).*$', '\\1', info_box$`Senator (party-state)`)
# change state abbreviations to full names
senator_state <- get_full_state_name(state_list)
# party: grab element after opening parenthesis and before dash
party_list <- sub('^.*\\((.*)\\-.*\\).*$', '\\1', info_box$`Senator (party-state)`)
# change R, D and I to full party names
# leave unchanged if none of these options are matched
senator_party <- ifelse(party_list=='R', 'Republican',
ifelse(party_list == 'D', 'Democratic',
ifelse(party_list == 'I', 'Independent', party_list)))}
# neat column formatting from 114th congress
else {
senator_name <- info_box$Senator
senator_state <- info_box$State
senator_party <- info_box$Party}
# adding data to a data frame for the year
congress_term <- data.frame(
# varying number of senators per term
congress = rep(term_suffix(term), length(senator_name)),
senator_name,
senator_state,
senator_party)
return(congress_term)}
# empty data frame to store results
us_congress <- data.frame()
# calling the function above for every congress term
for (term in seq(83, 118)) {
# extract dataset for each term
senators_term <- wiki_senate(term)
# add term-specific dataset to overall result dataset
us_congress <- rbind(us_congress, senators_term)}
# clean footnotes
# used chatgptfor regex
us_congress <- us_congress %>%
mutate(across(everything(), ~ str_remove_all(.x, "\\[.*?\\]")))
# sort by congress term (descending), by state (ascending) and then by senator name (ascending)
us_congress <- us_congress %>%
# the suffix causes issues with sorting since it's interpreted as a string
# temporarily create a column without suffix (last 2 characters)
mutate(congress_num = as.numeric(substr(congress, 1, nchar(congress) - 2))) %>%
arrange(desc(congress_num), senator_state, senator_name) %>%
# remove temporary column
select(-congress_num)
# save in 'ex2' folder as 'sens_df.csv'
write_csv(us_congress, 'sens_df.csv')
a <- wiki_senate(118)
a
a <- wiki_senate(108)
a
